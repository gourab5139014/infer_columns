{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import random\n",
    "from collections import Counter, namedtuple\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 26 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 40]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35  8 37 38 40]\n"
     ]
    }
   ],
   "source": [
    "fake1 = np.linspace(0, 40, 40, dtype=int)\n",
    "fake2 = fake1.copy()\n",
    "\n",
    "replace_at = np.random.randint(0, 40, size=1)\n",
    "for i in range(0, len(fake1)): # Replacing random elements in the uniform distribution\n",
    "    if(i in replace_at):\n",
    "        fake1[i] = np.random.randint(0, 40)\n",
    "print(fake1)\n",
    "\n",
    "fake2 = np.linspace(0, 40, 40, dtype=int)\n",
    "replace_at = np.random.randint(0, 40, size=1)\n",
    "for i in range(0, len(fake2)): # Replacing random elements in the uniform distribution\n",
    "    if(i in replace_at):\n",
    "        fake2[i] = np.random.randint(0, 40)\n",
    "print(fake2)\n",
    "# mini_fake1 = random.sample(fake1, 10)\n",
    "# print(mini_fake1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_div_scipy(p,q):\n",
    "    p = np.asarray(p, dtype=np.float)\n",
    "    q = np.asarray(q, dtype=np.float)\n",
    "\n",
    "#     return np.sum(np.where(p != 0, p * np.log(p / q), 0))\n",
    "    return st.entropy(p,q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.034556015664196998"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_div_scipy(fake1,fake2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(p,q):\n",
    "    \"\"\" Returns Kl Divergence of two integer lists. Theory at https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained\n",
    "    :type p: List[int]\n",
    "    :type q: List[int]\n",
    "    :rtype: double\n",
    "    \"\"\"\n",
    "    cf1, l1 = Counter(p), len(p)\n",
    "    cf2, l2 = Counter(q), len(q)\n",
    "    \n",
    "    # Pre-processing for using KL Divergence of Frequency Counters cf1 and cf2\n",
    "    s = set(cf1.keys())\n",
    "    s.intersection(cf2.keys()) # Collecting all unique elements in cf1 and cf2\n",
    "\n",
    "    # Normalizing the series to reflect probabilities of occurence\n",
    "    for e in cf1:\n",
    "        cf1[e] = float(cf1[e]/l1)\n",
    "    for e in cf2:\n",
    "        cf2[e] = float(cf2[e]/l2)\n",
    "    print(\"Kl Div from library func = {0}\".format(kl_div_scipy(list(cf1.values()),list(cf2.values()))))\n",
    "    kl_div = 0.0\n",
    "    for c in s: # For each unique element in both series\n",
    "        if cf1[c] != 0 and cf2[c] != 0:\n",
    "            t = cf1[c] * log(cf1[c]/cf2[c])\n",
    "#             print(\"cf1[{0}]={1} cf2[{0}]={2} t={3} k={4}\".format(c, cf1[c], cf2[c], t, kl_div))\n",
    "            kl_div += t \n",
    "            \n",
    "            t2 = cf2[c] * log(cf2[c]/cf1[c])\n",
    "#             print(\"cf2[{0}]={1} cf1[{0}]={2} t2={3} k={4}\".format(c, cf2[c], cf1[c], t2, kl_div))\n",
    "            kl_div += t2 \n",
    "    return kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kl Div from library func = 0.017328679513998628\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.034657359027997256"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_divergence(fake1, fake2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dist: <scipy.stats._continuous_distns.uniform_gen object at 0x0000016164ABE6D8>. Parameters: (-9.9936295516605124e-05, 40.000116111526083)\n",
      "Dist: <scipy.stats._continuous_distns.norm_gen object at 0x0000016164A3A5C0>. Parameters: (19.75, 11.622714829161042)\n",
      "Dist: <scipy.stats._continuous_distns.invgauss_gen object at 0x0000016164A882B0>. Parameters: (0.050102514050857791, -34.492910046934568, 1078.8852347800894)\n"
     ]
    }
   ],
   "source": [
    "data = fake1\n",
    "# Testing KL Divergence with a standard distribution\n",
    "DISTRIBUTIONS = [st.uniform, st.norm, st.invgauss]\n",
    "for distribution in DISTRIBUTIONS:\n",
    "    # fit dist to data\n",
    "    params = distribution.fit(data)\n",
    "    print(\"Dist: {0}. Parameters: {1}\".format(distribution, params))\n",
    "\n",
    "# Distribution.fit uses scipy.stats._continuous_distns.py --(uses)--> scipy.stats._distn_infrastructure.rv_continuous.fit() --(uses)--> scipy.optimize.optimize.fmin()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
